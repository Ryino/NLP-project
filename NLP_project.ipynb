{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO/xEpSx54wRmIgLu2rNlCF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "2021535 RAYYAN HASSAN\n",
        "2021152 Faakhir Inam"
      ],
      "metadata": {
        "id": "wu3wArumD0_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow tensorflow-text tensorflow-datasets tensorflow_decision_forests tensorflow-estimator tensorflow-hub tensorflow-io-gcs-filesystem tensorflow-metadata tensorflow-probability spacy fasttext numpy pandas scikit-learn nltk ml-dtypes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BakUt8WnmyHS",
        "outputId": "505aae2a-60b0-4078-f655-8d4dc2a4be67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.15.0\n",
            "Uninstalling tensorflow-2.15.0:\n",
            "  Successfully uninstalled tensorflow-2.15.0\n",
            "Found existing installation: tensorflow-text 2.19.0\n",
            "Uninstalling tensorflow-text-2.19.0:\n",
            "  Successfully uninstalled tensorflow-text-2.19.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-datasets as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow_decision_forests as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tensorflow-estimator 2.15.0\n",
            "Uninstalling tensorflow-estimator-2.15.0:\n",
            "  Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "\u001b[33mWARNING: Skipping tensorflow-hub as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tensorflow-io-gcs-filesystem 0.37.1\n",
            "Uninstalling tensorflow-io-gcs-filesystem-0.37.1:\n",
            "  Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1\n",
            "\u001b[33mWARNING: Skipping tensorflow-metadata as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-probability as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: spacy 3.7.4\n",
            "Uninstalling spacy-3.7.4:\n",
            "  Successfully uninstalled spacy-3.7.4\n",
            "Found existing installation: fasttext 0.9.2\n",
            "Uninstalling fasttext-0.9.2:\n",
            "  Successfully uninstalled fasttext-0.9.2\n",
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "Found existing installation: pandas 2.0.3\n",
            "Uninstalling pandas-2.0.3:\n",
            "  Successfully uninstalled pandas-2.0.3\n",
            "Found existing installation: scikit-learn 1.3.2\n",
            "Uninstalling scikit-learn-1.3.2:\n",
            "  Successfully uninstalled scikit-learn-1.3.2\n",
            "Found existing installation: nltk 3.8.1\n",
            "Uninstalling nltk-3.8.1:\n",
            "  Successfully uninstalled nltk-3.8.1\n",
            "Found existing installation: ml-dtypes 0.2.0\n",
            "Uninstalling ml-dtypes-0.2.0:\n",
            "  Successfully uninstalled ml-dtypes-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart runtime: Runtime > Restart runtime\n",
        "\n",
        "# Uninstall all TensorFlow-related packages and dependencies to avoid conflicts\n",
        "#!pip uninstall -y tensorflow tensorflow-text tensorflow-datasets tensorflow_decision_forests tensorflow-estimator tensorflow-hub tensorflow-io-gcs-filesystem tensorflow-metadata tensorflow-probability spacy fasttext numpy pandas scikit-learn nltk ml-dtypes\n",
        "\n",
        "# Install dependencies with strict versions\n",
        "#!pip install tensorflow==2.15.0 spacy==3.7.4 fasttext==0.9.2 numpy==1.23.5 pandas==2.0.3 scikit-learn==1.3.2 nltk==3.8.1\n",
        "!python -m nltk.downloader punkt\n",
        "\n",
        "# Clean up any incomplete or corrupted FastText files\n",
        "!rm -f /content/cc.ur.300.bin /content/cc.ur.300.bin.gz /content/cc.en.300.bin /content/cc.en.300.bin.gz\n",
        "\n",
        "# Download FastText models with progress monitoring\n",
        "!wget --progress=bar:force https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ur.300.bin.gz -O /content/cc.ur.300.bin.gz\n",
        "!wget --progress=bar:force https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz -O /content/cc.en.300.bin.gz\n",
        "\n",
        "# Unzip the models\n",
        "!gunzip /content/cc.ur.300.bin.gz\n",
        "!gunzip /content/cc.en.300.bin.gz\n",
        "\n",
        "# Verify file sizes (should be ~4.1GB each)\n",
        "!ls -lh /content/cc.ur.300.bin /content/cc.en.300.bin\n",
        "\n",
        "# Verify package versions\n",
        "!pip list | grep -E \"tensorflow|spacy|fasttext|numpy|pandas|scikit-learn|nltk|ml-dtypes\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FGahjrhIkc3",
        "outputId": "2c1c1a91-f028-4721-a13a-2f527f430b16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "--2025-05-12 16:39:43--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ur.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.171.22.68, 3.171.22.33, 3.171.22.13, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.171.22.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3581706168 (3.3G) [application/octet-stream]\n",
            "Saving to: ‘/content/cc.ur.300.bin.gz’\n",
            "\n",
            "/content/cc.ur.300. 100%[===================>]   3.33G  78.9MB/s    in 52s     \n",
            "\n",
            "2025-05-12 16:40:36 (66.1 MB/s) - ‘/content/cc.ur.300.bin.gz’ saved [3581706168/3581706168]\n",
            "\n",
            "--2025-05-12 16:40:36--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.171.22.118, 3.171.22.68, 3.171.22.33, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.171.22.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘/content/cc.en.300.bin.gz’\n",
            "\n",
            "/content/cc.en.300. 100%[===================>]   4.19G   280MB/s    in 14s     \n",
            "\n",
            "2025-05-12 16:40:50 (298 MB/s) - ‘/content/cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n",
            "-rw-r--r-- 1 root root 6.8G Jan 18  2019 /content/cc.en.300.bin\n",
            "-rw-r--r-- 1 root root 4.9G Jan 18  2019 /content/cc.ur.300.bin\n",
            "nltk                      3.9.1\n",
            "numpy                     2.0.2\n",
            "pandas                    2.2.2\n",
            "pandas-stubs              2.2.2.240909\n",
            "scikit-learn              1.6.1\n",
            "sklearn-pandas            2.2.0\n",
            "spacy                     3.8.5\n",
            "spacy-legacy              3.0.12\n",
            "spacy-loggers             1.0.5\n",
            "tensorflow-datasets       4.9.8\n",
            "tensorflow-metadata       1.17.1\n",
            "tensorflow-probability    0.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow tensorflow-text spacy fasttext numpy pandas scikit-learn nltk\n",
        "!python -m nltk.downloader punkt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFOuptKfkr2z",
        "outputId": "1514487c-a9e7-4a52-fc5c-384d5dea22dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m587.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313505 sha256=5beed0efadd9a07259ac5db6a7ac9fd98bfedf048f90794fecf1684023578998\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, pybind11, google-pasta, tensorboard, fasttext, astunparse, tensorflow, tensorflow-text\n",
            "Successfully installed astunparse-1.6.3 fasttext-0.9.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 pybind11-2.13.6 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-text-2.19.0 werkzeug-3.1.3 wheel-0.45.1\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7py4ZkpGjmSk",
        "outputId": "3d83d820-d31c-47a7-b5d1-c2a9a935fe5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import fasttext\n",
        "import os\n",
        "import math\n",
        "import functools\n",
        "import operator\n",
        "from collections import Counter\n",
        "import io\n",
        "import logging\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Setup minimal logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize Spacy NLP objects globally\n",
        "try:\n",
        "    nlp_ur = spacy.blank('ur')\n",
        "    nlp_en = spacy.blank('en')\n",
        "    logger.info(\"Spacy models initialized successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to initialize Spacy models: {e}\")\n",
        "    nlp_ur = None\n",
        "    nlp_en = None\n",
        "\n",
        "# --- Tokenization ---\n",
        "def urdu_tokens(inp):\n",
        "    if not isinstance(inp, str):\n",
        "        logger.warning(f\"Invalid input type for urdu_tokens. Converting to string.\")\n",
        "        inp = str(inp)\n",
        "    if len(inp) > 500:\n",
        "        logger.warning(f\"Truncating Urdu input exceeding 500 chars\")\n",
        "        inp = inp[:500]\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if nlp_ur:\n",
        "            doc = nlp_ur(inp)\n",
        "            tokens = [str(word) for word in doc]\n",
        "            return tokens\n",
        "        else:\n",
        "            logger.warning(\"Using fallback tokenizer for Urdu\")\n",
        "            return inp.strip().split()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in urdu_tokens: {e}\")\n",
        "        return inp.strip().split()  # Fallback on error\n",
        "    finally:\n",
        "        elapsed = time.time() - start_time\n",
        "        if elapsed > 5:\n",
        "            logger.warning(f\"Tokenization took {elapsed:.2f}s\")\n",
        "\n",
        "def eng_tokens(inp):\n",
        "    if not isinstance(inp, str):\n",
        "        logger.warning(f\"Invalid input type for eng_tokens. Converting to string.\")\n",
        "        inp = str(inp)\n",
        "    if len(inp) > 500:\n",
        "        logger.warning(f\"Truncating English input exceeding 500 chars\")\n",
        "        inp = inp[:500]\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if nlp_en:\n",
        "            doc = nlp_en(inp)\n",
        "            tokens = [str(word) for word in doc]\n",
        "            return tokens\n",
        "        else:\n",
        "            logger.warning(\"Using fallback tokenizer for English\")\n",
        "            return inp.strip().split()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in eng_tokens: {e}\")\n",
        "        return inp.strip().split()  # Fallback on error\n",
        "    finally:\n",
        "        elapsed = time.time() - start_time\n",
        "        if elapsed > 5:\n",
        "            logger.warning(f\"Tokenization took {elapsed:.2f}s\")\n",
        "\n",
        "def build_vocab(filepath, tokenizer, min_freq=1):\n",
        "    counter = Counter()\n",
        "    try:\n",
        "        with io.open(filepath, encoding='utf8', errors='ignore') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i % 1000 == 0:\n",
        "                    logger.info(f\"Processing line {i} in {filepath}\")\n",
        "                counter.update(tokenizer(line.strip()))\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading {filepath}: {e}\")\n",
        "        raise\n",
        "    vocab = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
        "    idx = 4\n",
        "    for token, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[token] = idx\n",
        "            idx += 1\n",
        "    logger.info(f\"Built vocab with {len(vocab)} tokens from {filepath}\")\n",
        "    return vocab, {v: k for k, v in vocab.items()}\n",
        "\n",
        "# --- Data Processing ---\n",
        "def load_data(urdu_file='/content/Urdu.txt', english_file='/content/English.txt', max_lines=10000):\n",
        "    try:\n",
        "        urdu_lines, english_lines = [], []\n",
        "        with open(urdu_file, 'r', encoding='utf8') as uf, open(english_file, 'r', encoding='utf8') as ef:\n",
        "            for ur_line, en_line in zip(uf, ef):\n",
        "                ur_line, en_line = ur_line.strip(), en_line.strip()\n",
        "                if not ur_line or not en_line or len(ur_line) > 500 or len(en_line) > 500:\n",
        "                    continue\n",
        "                urdu_lines.append(ur_line)\n",
        "                english_lines.append(en_line)\n",
        "                if len(urdu_lines) >= max_lines:\n",
        "                    break\n",
        "        assert len(urdu_lines) == len(english_lines), f\"Mismatched number of lines: {len(urdu_lines)} Urdu vs {len(english_lines)} English\"\n",
        "        logger.info(f\"Loaded {len(urdu_lines)} sentence pairs\")\n",
        "        return urdu_lines, english_lines\n",
        "    except FileNotFoundError as e:\n",
        "        logger.error(f\"Dataset file not found: {e}\")\n",
        "        raise\n",
        "    except UnicodeDecodeError as e:\n",
        "        logger.error(f\"Encoding error in dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "def create_splits(urdu_lines, english_lines):\n",
        "    train_size = int(0.7 * len(urdu_lines))\n",
        "    test_size = int(0.15 * len(urdu_lines))\n",
        "    val_size = len(urdu_lines) - train_size - test_size\n",
        "\n",
        "    urdu_train, urdu_remain = urdu_lines[:train_size], urdu_lines[train_size:]\n",
        "    eng_train, eng_remain = english_lines[:train_size], english_lines[train_size:]\n",
        "    urdu_val, urdu_test = urdu_remain[:val_size], urdu_remain[val_size:]\n",
        "    eng_val, eng_test = eng_remain[:val_size], eng_remain[val_size:]\n",
        "\n",
        "    logger.info(f\"Split sizes: {len(urdu_train)} train, {len(urdu_val)} val, {len(urdu_test)} test\")\n",
        "    return (urdu_train, eng_train), (urdu_val, eng_val), (urdu_test, eng_test)\n",
        "\n",
        "def preprocess_data(urdu_sentences, english_sentences, urdu_vocab, eng_vocab, max_len=50):\n",
        "    def tokenize_and_convert(sentences, vocab, tokenizer):\n",
        "        data = []\n",
        "        for i, s in enumerate(sentences):\n",
        "            if i % 1000 == 0:\n",
        "                logger.info(f\"Tokenizing sentence {i} of {len(sentences)}\")\n",
        "            tokens = ['<bos>'] + tokenizer(s)[:max_len-2] + ['<eos>']\n",
        "            ids = [vocab.get(t, vocab['<unk>']) for t in tokens]\n",
        "            ids += [vocab['<pad>']] * (max_len - len(ids))\n",
        "            data.append(ids[:max_len])\n",
        "        return np.array(data, dtype=np.int32)\n",
        "\n",
        "    urdu_token_ids = tokenize_and_convert(urdu_sentences, urdu_vocab, urdu_tokens)\n",
        "    eng_token_ids = tokenize_and_convert(english_sentences, eng_vocab, eng_tokens)\n",
        "    logger.info(f\"Preprocessed {len(urdu_token_ids)} Urdu and {len(eng_token_ids)} English sequences\")\n",
        "    return urdu_token_ids, eng_token_ids\n",
        "\n",
        "# --- FastText Embeddings ---\n",
        "def load_fasttext_embeddings(urdu_model_path='/content/cc.ur.300.bin', english_model_path='/content/cc.en.300.bin'):\n",
        "    try:\n",
        "        if not os.path.exists(urdu_model_path):\n",
        "            logger.error(f\"Urdu FastText model not found at {urdu_model_path}\")\n",
        "            raise FileNotFoundError(f\"Missing {urdu_model_path}\")\n",
        "        if not os.path.exists(english_model_path):\n",
        "            logger.error(f\"English FastText model not found at {english_model_path}\")\n",
        "            raise FileNotFoundError(f\"Missing {english_model_path}\")\n",
        "        urdu_model = fasttext.load_model(urdu_model_path)\n",
        "        english_model = fasttext.load_model(english_model_path)\n",
        "        logger.info(\"FastText models loaded successfully\")\n",
        "        return urdu_model, english_model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load FastText models: {e}. Proceeding without embeddings.\")\n",
        "        return None, None  # Fallback to baseline model\n",
        "\n",
        "def create_embedding_matrix(vocab, fasttext_model, embedding_dim=300):\n",
        "    if fasttext_model is None:\n",
        "        logger.warning(\"No FastText model provided. Returning zero matrix.\")\n",
        "        return np.zeros((len(vocab), embedding_dim))\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
        "    missing_tokens = 0\n",
        "    for token, idx in vocab.items():\n",
        "        if token in ['<pad>', '<unk>', '<bos>', '<eos>']:\n",
        "            continue\n",
        "        try:\n",
        "            embedding_matrix[idx] = fasttext_model.get_word_vector(token)\n",
        "        except KeyError:\n",
        "            missing_tokens += 1\n",
        "            embedding_matrix[idx] = np.zeros(embedding_dim)  # Initialize missing tokens with zeros\n",
        "    logger.info(f\"Created embedding matrix for vocab size {len(vocab)} with shape {embedding_matrix.shape}, {missing_tokens} missing tokens\")\n",
        "    return embedding_matrix\n",
        "\n",
        "# --- Transformer Model ---\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=300, num_layers=3, num_heads=8, dff=512, max_len=50, src_embedding_matrix=None, tgt_embedding_matrix=None, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Embeddings\n",
        "        if src_embedding_matrix is not None and not np.all(src_embedding_matrix == 0):\n",
        "            logger.info(f\"Using src_embedding_matrix with shape {src_embedding_matrix.shape} for src_vocab_size={src_vocab_size}\")\n",
        "            self.src_embedding = tf.keras.layers.Embedding(src_vocab_size, d_model, weights=[src_embedding_matrix], trainable=False)\n",
        "        else:\n",
        "            self.src_embedding = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "\n",
        "        if tgt_embedding_matrix is not None and not np.all(tgt_embedding_matrix == 0):\n",
        "            logger.info(f\"Using tgt_embedding_matrix with shape {tgt_embedding_matrix.shape} for tgt_vocab_size={tgt_vocab_size}\")\n",
        "            self.tgt_embedding = tf.keras.layers.Embedding(tgt_vocab_size, d_model, weights=[tgt_embedding_matrix], trainable=False)\n",
        "        else:\n",
        "            self.tgt_embedding = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        # Encoder and Decoder layers\n",
        "        self.encoder_layers = [self.encoder_layer(d_model, num_heads, dff, dropout) for _ in range(num_layers)]\n",
        "        self.decoder_layers = [self.decoder_layer(d_model, num_heads, dff, dropout) for _ in range(num_layers)]\n",
        "\n",
        "        # Output layer\n",
        "        self.final_layer = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "    def positional_encoding(self, length, depth):\n",
        "        depth = depth / 2\n",
        "        positions = np.arange(length)[:, np.newaxis]\n",
        "        depths = np.arange(depth)[np.newaxis, :] / depth\n",
        "        angle_rates = 1 / (10000 ** depths)\n",
        "        angle_rads = positions * angle_rates\n",
        "        pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def encoder_layer(self, d_model, num_heads, dff, dropout):\n",
        "        inputs = tf.keras.Input(shape=(None, d_model))\n",
        "        attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(inputs, inputs)\n",
        "        attn_output = tf.keras.layers.Dropout(dropout)(attn_output)\n",
        "        out1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)(inputs + attn_output)\n",
        "        ffn_output = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])(out1)\n",
        "        ffn_output = tf.keras.layers.Dropout(dropout)(ffn_output)\n",
        "        outputs = tf.keras.layers.LayerNormalization(epsilon=1e-5)(out1 + ffn_output)\n",
        "        return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    def decoder_layer(self, d_model, num_heads, dff, dropout):\n",
        "        inputs = tf.keras.Input(shape=(None, d_model))\n",
        "        enc_outputs = tf.keras.Input(shape=(None, d_model))\n",
        "        self_attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(inputs, inputs)\n",
        "        self_attn = tf.keras.layers.Dropout(dropout)(self_attn)\n",
        "        out1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)(inputs + self_attn)\n",
        "        cross_attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(out1, enc_outputs)\n",
        "        cross_attn = tf.keras.layers.Dropout(dropout)(cross_attn)\n",
        "        out2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)(out1 + cross_attn)\n",
        "        ffn_output = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])(out2)\n",
        "        ffn_output = tf.keras.layers.Dropout(dropout)(ffn_output)\n",
        "        outputs = tf.keras.layers.LayerNormalization(epsilon=1e-5)(out2 + ffn_output)\n",
        "        return tf.keras.Model(inputs=[inputs, enc_outputs], outputs=outputs)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        src, tgt = inputs\n",
        "        src_emb = self.src_embedding(src) * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        src_emb += self.pos_encoding[:tf.shape(src)[1], :]\n",
        "        src_emb = self.dropout(src_emb, training=training)\n",
        "\n",
        "        tgt_emb = self.tgt_embedding(tgt) * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        tgt_emb += self.pos_encoding[:tf.shape(tgt)[1], :]\n",
        "        tgt_emb = self.dropout(tgt_emb, training=training)\n",
        "\n",
        "        enc_output = src_emb\n",
        "        for layer in self.encoder_layers:\n",
        "            enc_output = layer(enc_output, training=training)\n",
        "\n",
        "        dec_output = tgt_emb\n",
        "        for layer in self.decoder_layers:\n",
        "            dec_output = layer([dec_output, enc_output], training=training)\n",
        "\n",
        "        logits = self.final_layer(dec_output)\n",
        "        return logits\n",
        "\n",
        "# --- BLEU Score Calculation ---\n",
        "def get_ngrams(sentence, n):\n",
        "    words = sentence.strip().split()\n",
        "    ngrams = {}\n",
        "    for i in range(len(words) - n + 1):\n",
        "        ngram = ' '.join(words[i:i+n]).lower()\n",
        "        ngrams[ngram] = ngrams.get(ngram, 0) + 1\n",
        "    return ngrams, len(words)\n",
        "\n",
        "def compute_bleu(candidate_sentences, reference_sentences, max_n=4):\n",
        "    precisions = []\n",
        "    for n in range(1, max_n+1):\n",
        "        candref_count = 0\n",
        "        count = 0\n",
        "        ref_count = 0\n",
        "        cand_count = 0\n",
        "        for cand, refs in zip(candidate_sentences, reference_sentences):\n",
        "            cand_ngrams, cand_len = get_ngrams(cand, n)\n",
        "            ref_ngrams_list = [get_ngrams(ref, n)[0] for ref in refs]\n",
        "            ref_lengths = [get_ngrams(ref, n)[1] for ref in refs]\n",
        "\n",
        "            candref_temp = 0\n",
        "            for ngram, freq in cand_ngrams.items():\n",
        "                max_ref_freq = max([ref_ngrams.get(ngram, 0) for ref_ngrams in ref_ngrams_list], default=0)\n",
        "                candref_temp += min(freq, max_ref_freq)\n",
        "            candref_count += candref_temp\n",
        "            count += max(0, cand_len - n + 1)\n",
        "            cand_count += cand_len\n",
        "            ref_count += min(ref_lengths, key=lambda x: abs(cand_len - x))\n",
        "\n",
        "        pr = candref_count / count if count > 0 else 0\n",
        "        precisions.append(pr)\n",
        "\n",
        "    if cand_count > ref_count:\n",
        "        penalty = 1\n",
        "    else:\n",
        "        penalty = math.exp(1 - (float(ref_count) / cand_count)) if cand_count > 0 else 0\n",
        "\n",
        "    if all(p > 0 for p in precisions):\n",
        "        bleu = (functools.reduce(operator.mul, precisions)) ** (1.0 / len(precisions)) * penalty\n",
        "    else:\n",
        "        bleu = 0\n",
        "    return bleu\n",
        "\n",
        "# --- Training ---\n",
        "def train_model(model, src_tokens, tgt_tokens, epochs=3, batch_size=64):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(src_batch, tgt_batch):\n",
        "        tgt_input = tgt_batch[:, :-1]\n",
        "        tgt_target = tgt_batch[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model([src_batch, tgt_input], training=True)\n",
        "            loss = loss_fn(tgt_target, logits)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((src_tokens, tgt_tokens))\n",
        "    dataset = dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        steps = 0\n",
        "        for src_batch, tgt_batch in dataset:\n",
        "            loss = train_step(src_batch, tgt_batch)\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/steps:.4f}\")\n",
        "\n",
        "# --- Greedy Decoding ---\n",
        "def greedy_decode(model, src, max_len, start_symbol, pad_symbol, end_symbol, vocab_size):\n",
        "    src = tf.convert_to_tensor(src)\n",
        "    ys = tf.ones((1, 1), dtype=tf.int32) * start_symbol\n",
        "    for _ in range(max_len-1):\n",
        "        logits = model([src, ys], training=False)\n",
        "        next_word = tf.cast(tf.argmax(logits[:, -1, :], axis=-1), tf.int32)\n",
        "        ys = tf.concat([ys, next_word[:, tf.newaxis]], axis=1)\n",
        "        if next_word.numpy()[0] == end_symbol:\n",
        "            break\n",
        "    return ys.numpy()[0]\n",
        "\n",
        "def translate(model, src_sentence, src_vocab, tgt_vocab, src_tokenizer, max_len=50):\n",
        "    tokens = ['<bos>'] + src_tokenizer(src_sentence) + ['<eos>']\n",
        "    src_ids = [src_vocab.get(t, src_vocab['<unk>']) for t in tokens]\n",
        "    src_ids = src_ids[:max_len] + [src_vocab['<pad>']] * (max_len - len(src_ids))\n",
        "    src = np.array([src_ids], dtype=np.int32)\n",
        "    tgt_ids = greedy_decode(model, src, max_len, src_vocab['<bos>'], src_vocab['<pad>'], src_vocab['<eos>'], len(tgt_vocab))\n",
        "    return ' '.join([tgt_vocab.get(id, '<unk>') for id in tgt_ids]).replace('<bos>', '').replace('<eos>', '').strip()\n",
        "\n",
        "# --- BLEU Score Calculation ---\n",
        "def prepare_bleu_data(model, src_tokens, tgt_sentences):\n",
        "    translations = []\n",
        "    for i in range(len(src_tokens)):\n",
        "        if i % 100 == 0:\n",
        "            logger.info(f\"Translating sentence {i} of {len(src_tokens)}\")\n",
        "        try:\n",
        "            trans = translate(model, urdu_lines[i], urdu_vocab, eng_reverse_vocab, urdu_tokens)\n",
        "            translations.append(trans)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error translating sentence {i}: {e}\")\n",
        "            translations.append(\"\")  # Fallback empty translation\n",
        "    references = [[s] for s in tgt_sentences]\n",
        "    logger.info(f\"Generated {len(translations)} translations\")\n",
        "    return translations, references\n",
        "\n",
        "# --- Main Execution ---\n",
        "def main():\n",
        "    # Load and split data\n",
        "    urdu_lines, english_lines = load_data()\n",
        "    (urdu_train, eng_train), (urdu_val, eng_val), (urdu_test, eng_test) = create_splits(urdu_lines, english_lines)\n",
        "\n",
        "    # Save train data for vocab building\n",
        "    with open('/content/urdu_train.txt', 'w', encoding='utf8') as f:\n",
        "        for line in urdu_train:\n",
        "            f.write(line + '\\n')\n",
        "    with open('/content/english_train.txt', 'w', encoding='utf8') as f:\n",
        "        for line in eng_train:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "    # Build vocabularies\n",
        "    urdu_vocab, urdu_reverse_vocab = build_vocab('/content/urdu_train.txt', urdu_tokens)\n",
        "    eng_vocab, eng_reverse_vocab = build_vocab('/content/english_train.txt', eng_tokens)\n",
        "    logger.info(f\"Urdu vocab size: {len(urdu_vocab)}, English vocab size: {len(eng_vocab)}\")\n",
        "\n",
        "    # Preprocess data\n",
        "    max_len = 50\n",
        "    urdu_train_token_ids, eng_train_token_ids = preprocess_data(urdu_train, eng_train, urdu_vocab, eng_vocab, max_len)\n",
        "    urdu_val_token_ids, eng_val_token_ids = preprocess_data(urdu_val, eng_val, urdu_vocab, eng_vocab, max_len)\n",
        "    urdu_test_token_ids, eng_test_token_ids = preprocess_data(urdu_test, eng_test, urdu_vocab, eng_vocab, max_len)\n",
        "\n",
        "    # Load FastText embeddings\n",
        "    urdu_fasttext, eng_fasttext = load_fasttext_embeddings()\n",
        "    urdu_embedding_matrix = create_embedding_matrix(urdu_vocab, urdu_fasttext)\n",
        "    eng_embedding_matrix = create_embedding_matrix(eng_vocab, eng_fasttext)\n",
        "\n",
        "    # Initialize models\n",
        "    baseline_model = Transformer(\n",
        "        src_vocab_size=len(urdu_vocab),\n",
        "        tgt_vocab_size=len(eng_vocab),\n",
        "        d_model=300,\n",
        "        num_layers=3,\n",
        "        num_heads=8,\n",
        "        dff=512,\n",
        "        max_len=max_len\n",
        "    )\n",
        "    extended_model = Transformer(\n",
        "        src_vocab_size=len(urdu_vocab),\n",
        "        tgt_vocab_size=len(eng_vocab),\n",
        "        d_model=300,\n",
        "        num_layers=3,\n",
        "        num_heads=8,\n",
        "        dff=512,\n",
        "        max_len=max_len,\n",
        "        src_embedding_matrix=urdu_embedding_matrix,\n",
        "        tgt_embedding_matrix=eng_embedding_matrix\n",
        "    )\n",
        "\n",
        "    # Train models\n",
        "    print(\"Training Baseline Model...\")\n",
        "    train_model(baseline_model, urdu_train_token_ids, eng_train_token_ids, epochs=3)\n",
        "    print(\"Training Extended Model...\")\n",
        "    train_model(extended_model, urdu_train_token_ids, eng_train_token_ids, epochs=3)\n",
        "\n",
        "    # Evaluate BLEU\n",
        "    print(\"Evaluating Baseline Model...\")\n",
        "    baseline_trans, baseline_refs = prepare_bleu_data(baseline_model, urdu_test_token_ids, eng_test)\n",
        "    print(\"Evaluating Extended Model...\")\n",
        "    extended_trans, extended_refs = prepare_bleu_data(extended_model, urdu_test_token_ids, eng_test)\n",
        "\n",
        "    baseline_bleu = compute_bleu(baseline_trans, baseline_refs)\n",
        "    extended_bleu = compute_bleu(extended_trans, extended_refs)\n",
        "    print(f\"Baseline BLEU Score: {baseline_bleu:.4f}\")\n",
        "    print(f\"Extended BLEU Score (with FastText): {extended_bleu:.4f}\")\n",
        "\n",
        "    # Save translations\n",
        "    with open('/content/translations.txt', 'w', encoding='utf8') as f:\n",
        "        for src, trans in zip(urdu_test[:25], extended_trans[:25]):\n",
        "            f.write(f\"Source: {src}\\nTranslation: {trans}\\n\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dQDYxbbXH_lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4V5kYsTHKt2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}